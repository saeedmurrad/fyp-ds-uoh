{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":4104,"databundleVersionId":46661,"isSourceIdPinned":false},{"sourceType":"datasetVersion","sourceId":2314637,"datasetId":1396792,"databundleVersionId":2356056},{"sourceType":"datasetVersion","sourceId":7866129,"datasetId":4614938,"databundleVersionId":7971356}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom imblearn.over_sampling import SMOTE\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\n# ----------------------------\n# Paths (YOUR paths)\n# ----------------------------\ntrain_csv_path = \"/kaggle/input/diabetic-retinopathy-detection/trainLabels.csv.zip\"\ntrain_images_dir = \"/kaggle/input/datasets/josephrynkiewicz/diabetic-retinopathy-train-unzipped/train\"\n\n# ----------------------------\n# Load CSV\n# ----------------------------\ndf = pd.read_csv(train_csv_path)\n\nimg_number = 10016\ndf = df.iloc[:img_number].copy()\n\n# Binary label\ndf[\"Label\"] = (df[\"level\"] != 0).astype(int)\n\n# Build filepath\ndf[\"filepath\"] = df[\"image\"].apply(lambda x: os.path.join(train_images_dir, f\"{x}.jpeg\"))\n\n# Keep 10%\ndf = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n\n# ----------------------------\n# Skip missing/corrupt files\n# ----------------------------\ndf = df[df[\"filepath\"].apply(os.path.exists)].reset_index(drop=True)\n\ndef is_readable(path):\n    try:\n        im = cv2.imread(path)\n        return im is not None\n    except Exception:\n        return False\n\ndf = df[df[\"filepath\"].apply(is_readable)].reset_index(drop=True)\nprint(\"Usable rows after filtering:\", len(df))\nprint(df[\"Label\"].value_counts())\n\n# ----------------------------\n# Split AFTER filtering\n# ----------------------------\ntrain_df, temp_df = train_test_split(df, test_size=0.4, stratify=df[\"Label\"], random_state=42)\nval_df, test_df   = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"Label\"], random_state=42)\n\nprint(\"Train:\", train_df.shape, \"Val:\", val_df.shape, \"Test:\", test_df.shape)\n\n\n# ============================================================\n# Step 2: Enhancements + Resize to 500x500\n# ============================================================\nIMG_SIZE = (500, 500)  # (H, W)\n\ndef enhance_none(img_rgb):\n    return img_rgb\n\ndef enhance_clahe(img_rgb):\n    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    l2 = clahe.apply(l)\n    out = cv2.merge((l2, a, b))\n    return cv2.cvtColor(out, cv2.COLOR_LAB2RGB)\n\ndef enhance_unsharp(img_rgb):\n    blur = cv2.GaussianBlur(img_rgb, (0, 0), sigmaX=1.0)\n    sharp = cv2.addWeighted(img_rgb, 1.5, blur, -0.5, 0)\n    return sharp\n\ndef enhance_edges(img_rgb):\n    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n    edges = cv2.Canny(gray, 50, 150)\n    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n    return cv2.addWeighted(img_rgb, 0.9, edges_rgb, 0.1, 0)\n\nENHANCERS = {\n    \"none\": enhance_none,\n    \"clahe\": enhance_clahe,\n    \"unsharp\": enhance_unsharp,\n    \"edges\": enhance_edges,\n}\n\ndef load_image_cv2(filepath, enhancer_name=\"none\"):\n    \"\"\"\n    Safe loader. Returns float32 [0,1] image or None.\n    NOTE: NO extra rescale later (avoid double normalization).\n    \"\"\"\n    try:\n        img = cv2.imread(filepath)\n        if img is None:\n            return None\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n        img = ENHANCERS[enhancer_name](img)\n        img = img.astype(np.float32) / 255.0\n        return img\n    except Exception:\n        return None\n\n\n# ============================================================\n# Step 1 + Step 3 (fast): embeddings + SMOTE + classifier\n# ============================================================\ndef build_feature_extractor(input_shape=(500, 500, 3)):\n    base = tf.keras.applications.EfficientNetB0(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=input_shape,\n        pooling=\"avg\"\n    )\n    base.trainable = False\n    return base\n\ndef compute_embeddings(df_subset, extractor, enhancer_name, batch_size=16):\n    paths = df_subset[\"filepath\"].tolist()\n    labels = df_subset[\"Label\"].values.astype(int)\n\n    X_chunks, y_chunks = [], []\n\n    for i in range(0, len(paths), batch_size):\n        batch_paths = paths[i:i+batch_size]\n        batch_y = labels[i:i+batch_size]\n\n        imgs, ys = [], []\n        for p, y in zip(batch_paths, batch_y):\n            im = load_image_cv2(p, enhancer_name=enhancer_name)\n            if im is not None:\n                imgs.append(im)\n                ys.append(y)\n\n        if not imgs:\n            continue\n\n        arr = np.stack(imgs, axis=0)\n        arr = tf.keras.applications.efficientnet.preprocess_input(arr * 255.0)\n\n        emb = extractor.predict(arr, verbose=0)\n        X_chunks.append(emb)\n        y_chunks.append(np.array(ys, dtype=int))\n\n    return np.concatenate(X_chunks, axis=0), np.concatenate(y_chunks, axis=0)\n\ndef train_smote_classifier(Xtr, ytr, Xva, yva, epochs=10):\n    sm = SMOTE(random_state=42)\n    Xb, yb = sm.fit_resample(Xtr, ytr)\n\n    clf = tf.keras.Sequential([\n        layers.Input(shape=(Xb.shape[1],)),\n        layers.Dense(256, activation=\"relu\"),\n        layers.Dropout(0.4),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation=\"sigmoid\")\n    ])\n    clf.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-3),\n        loss=\"binary_crossentropy\",\n        metrics=[tf.keras.metrics.AUC(name=\"auc\"), \"accuracy\"]\n    )\n    clf.fit(Xb, yb, validation_data=(Xva, yva), epochs=epochs, batch_size=64, verbose=1)\n    return clf\n\nextractor = build_feature_extractor((IMG_SIZE[0], IMG_SIZE[1], 3))\n\nresults = {}\nbest_enh, best_auc = None, -1\n\nfor enh in ENHANCERS:\n    print(f\"\\n=== Evaluating enhancement: {enh} ===\")\n    Xtr, ytr = compute_embeddings(train_df, extractor, enhancer_name=enh)\n    Xva, yva = compute_embeddings(val_df, extractor, enhancer_name=enh)\n\n    clf = train_smote_classifier(Xtr, ytr, Xva, yva, epochs=10)\n    pred = clf.predict(Xva, verbose=0).ravel()\n    auc = roc_auc_score(yva, pred)\n\n    results[enh] = float(auc)\n    print(f\"Validation AUC ({enh}): {auc:.4f}\")\n\n    if auc > best_auc:\n        best_auc = auc\n        best_enh = enh\n\nprint(\"\\nEnhancement comparison (AUC):\")\nfor k, v in sorted(results.items(), key=lambda x: x[1], reverse=True):\n    print(f\"{k:>8}: {v:.4f}\")\nprint(f\"\\nBest enhancement: {best_enh} (AUC={best_auc:.4f})\")\n\n\n# ============================================================\n# End-to-end EfficientNet training (FIXED decode error)\n# ============================================================\ndef make_tf_dataset(df_subset, enhancer_name, batch_size=8, training=False):\n    paths = df_subset[\"filepath\"].astype(str).values\n    labels = df_subset[\"Label\"].astype(np.float32).values\n\n    def _py_load(path_tensor):\n        # FIX: path_tensor is an EagerTensor -> convert to bytes -> decode\n        p = path_tensor.numpy().decode(\"utf-8\")\n        img = load_image_cv2(p, enhancer_name=enhancer_name)\n        if img is None:\n            img = np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32)\n        return img\n\n    def _load(path, label):\n        img = tf.py_function(_py_load, [path], Tout=tf.float32)\n        img.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n        img = tf.keras.applications.efficientnet.preprocess_input(img * 255.0)\n        return img, label\n\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n    if training:\n        ds = ds.shuffle(512, reshuffle_each_iteration=True)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = make_tf_dataset(train_df, best_enh, batch_size=8, training=True)\nval_ds   = make_tf_dataset(val_df, best_enh, batch_size=8, training=False)\ntest_ds  = make_tf_dataset(test_df, best_enh, batch_size=8, training=False)\n\n# Class weights\nneg = int((train_df[\"Label\"] == 0).sum())\npos = int((train_df[\"Label\"] == 1).sum())\nclass_weight = {0: (neg + pos) / (2.0 * neg), 1: (neg + pos) / (2.0 * pos)}\nprint(\"Class weight:\", class_weight)\n\ndef build_efficientnet(input_shape=(500, 500, 3), base_trainable=False):\n    base = tf.keras.applications.EfficientNetB0(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=input_shape,\n        pooling=\"avg\"\n    )\n    base.trainable = base_trainable\n\n    inp = layers.Input(shape=input_shape)\n    x = base(inp, training=False)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(1, activation=\"sigmoid\")(x)\n\n    model = tf.keras.Model(inp, out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss=\"binary_crossentropy\",\n        metrics=[tf.keras.metrics.AUC(name=\"auc\"), \"accuracy\"]\n    )\n    return model\n\nmodel = build_efficientnet((IMG_SIZE[0], IMG_SIZE[1], 3), base_trainable=False)\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=2, factor=0.5),\n]\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10,\n    class_weight=class_weight,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Test\ntest_pred = model.predict(test_ds, verbose=0).ravel()\ntest_y = test_df[\"Label\"].values.astype(int)\n\nprint(\"\\nTEST AUC:\", roc_auc_score(test_y, test_pred))\nprint(\"\\nClassification report (threshold=0.5):\")\nprint(classification_report(test_y, (test_pred >= 0.5).astype(int)))\nprint(\"Confusion matrix:\\n\", confusion_matrix(test_y, (test_pred >= 0.5).astype(int)))\n\nmodel.save(f\"efficientnet_best_{best_enh}_500x500.h5\")\nprint(f\"\\nSaved: efficientnet_best_{best_enh}_500x500.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T01:14:33.978911Z","iopub.execute_input":"2026-02-25T01:14:33.979288Z","iopub.status.idle":"2026-02-25T01:28:58.162864Z","shell.execute_reply.started":"2026-02-25T01:14:33.979259Z","shell.execute_reply":"2026-02-25T01:28:58.162263Z"}},"outputs":[{"name":"stdout","text":"Usable rows after filtering: 1002\nLabel\n0    736\n1    266\nName: count, dtype: int64\nTrain: (601, 4) Val: (200, 4) Test: (201, 4)\n\n=== Evaluating enhancement: none ===\nEpoch 1/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 211ms/step - accuracy: 0.4701 - auc: 0.5049 - loss: 0.7437 - val_accuracy: 0.7350 - val_auc: 0.5998 - val_loss: 0.6286\nEpoch 2/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5932 - auc: 0.6211 - loss: 0.6747 - val_accuracy: 0.4700 - val_auc: 0.5834 - val_loss: 0.7100\nEpoch 3/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6228 - auc: 0.6740 - loss: 0.6494 - val_accuracy: 0.6100 - val_auc: 0.6142 - val_loss: 0.6579\nEpoch 4/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6530 - auc: 0.7086 - loss: 0.6201 - val_accuracy: 0.7250 - val_auc: 0.6246 - val_loss: 0.5992\nEpoch 5/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6888 - auc: 0.7648 - loss: 0.5810 - val_accuracy: 0.5700 - val_auc: 0.6420 - val_loss: 0.6954\nEpoch 6/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6771 - auc: 0.7466 - loss: 0.5947 - val_accuracy: 0.6800 - val_auc: 0.6528 - val_loss: 0.6263\nEpoch 7/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7374 - auc: 0.8101 - loss: 0.5432 - val_accuracy: 0.7050 - val_auc: 0.6758 - val_loss: 0.6125\nEpoch 8/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7522 - auc: 0.8273 - loss: 0.5190 - val_accuracy: 0.6950 - val_auc: 0.6712 - val_loss: 0.6368\nEpoch 9/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7655 - auc: 0.8500 - loss: 0.4864 - val_accuracy: 0.6100 - val_auc: 0.6764 - val_loss: 0.6875\nEpoch 10/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7524 - auc: 0.8453 - loss: 0.4939 - val_accuracy: 0.6800 - val_auc: 0.6774 - val_loss: 0.6441\nValidation AUC (none): 0.6782\n\n=== Evaluating enhancement: clahe ===\nEpoch 1/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.5247 - auc: 0.5204 - loss: 0.7229 - val_accuracy: 0.2850 - val_auc: 0.6730 - val_loss: 0.8095\nEpoch 2/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5769 - auc: 0.6347 - loss: 0.6782 - val_accuracy: 0.5250 - val_auc: 0.6437 - val_loss: 0.7161\nEpoch 3/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6621 - auc: 0.7125 - loss: 0.6252 - val_accuracy: 0.5050 - val_auc: 0.6420 - val_loss: 0.7431\nEpoch 4/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6815 - auc: 0.7314 - loss: 0.6147 - val_accuracy: 0.6250 - val_auc: 0.6557 - val_loss: 0.6597\nEpoch 5/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7113 - auc: 0.7849 - loss: 0.5684 - val_accuracy: 0.6100 - val_auc: 0.6600 - val_loss: 0.6800\nEpoch 6/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7154 - auc: 0.7818 - loss: 0.5658 - val_accuracy: 0.7150 - val_auc: 0.6528 - val_loss: 0.5758\nEpoch 7/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7298 - auc: 0.8292 - loss: 0.5317 - val_accuracy: 0.6900 - val_auc: 0.6585 - val_loss: 0.6193\nEpoch 8/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7248 - auc: 0.8116 - loss: 0.5382 - val_accuracy: 0.6500 - val_auc: 0.6624 - val_loss: 0.6713\nEpoch 9/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7928 - auc: 0.8737 - loss: 0.4656 - val_accuracy: 0.6800 - val_auc: 0.6643 - val_loss: 0.6377\nEpoch 10/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7872 - auc: 0.8756 - loss: 0.4464 - val_accuracy: 0.7000 - val_auc: 0.6674 - val_loss: 0.6299\nValidation AUC (clahe): 0.6668\n\n=== Evaluating enhancement: unsharp ===\nEpoch 1/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 211ms/step - accuracy: 0.5275 - auc: 0.5283 - loss: 0.7141 - val_accuracy: 0.5650 - val_auc: 0.5886 - val_loss: 0.6785\nEpoch 2/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5758 - auc: 0.6244 - loss: 0.6705 - val_accuracy: 0.5950 - val_auc: 0.5809 - val_loss: 0.6662\nEpoch 3/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6329 - auc: 0.7010 - loss: 0.6296 - val_accuracy: 0.5950 - val_auc: 0.6144 - val_loss: 0.6625\nEpoch 4/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6851 - auc: 0.7455 - loss: 0.5979 - val_accuracy: 0.6200 - val_auc: 0.6510 - val_loss: 0.6453\nEpoch 5/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6853 - auc: 0.7503 - loss: 0.5914 - val_accuracy: 0.7150 - val_auc: 0.6490 - val_loss: 0.5936\nEpoch 6/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7334 - auc: 0.8106 - loss: 0.5358 - val_accuracy: 0.6550 - val_auc: 0.6572 - val_loss: 0.6432\nEpoch 7/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7286 - auc: 0.8162 - loss: 0.5240 - val_accuracy: 0.5850 - val_auc: 0.6783 - val_loss: 0.7376\nEpoch 8/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7460 - auc: 0.8245 - loss: 0.5216 - val_accuracy: 0.7250 - val_auc: 0.6706 - val_loss: 0.6001\nEpoch 9/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7645 - auc: 0.8549 - loss: 0.4805 - val_accuracy: 0.7200 - val_auc: 0.6796 - val_loss: 0.6017\nEpoch 10/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7871 - auc: 0.8724 - loss: 0.4446 - val_accuracy: 0.7250 - val_auc: 0.6516 - val_loss: 0.6236\nValidation AUC (unsharp): 0.6506\n\n=== Evaluating enhancement: edges ===\nEpoch 1/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.5313 - auc: 0.5470 - loss: 0.7002 - val_accuracy: 0.5850 - val_auc: 0.5832 - val_loss: 0.6872\nEpoch 2/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5766 - auc: 0.6324 - loss: 0.6678 - val_accuracy: 0.6100 - val_auc: 0.6242 - val_loss: 0.6702\nEpoch 3/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6335 - auc: 0.7041 - loss: 0.6263 - val_accuracy: 0.6300 - val_auc: 0.6432 - val_loss: 0.6440\nEpoch 4/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6783 - auc: 0.7338 - loss: 0.6114 - val_accuracy: 0.6100 - val_auc: 0.6708 - val_loss: 0.6647\nEpoch 5/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6984 - auc: 0.7716 - loss: 0.5840 - val_accuracy: 0.6400 - val_auc: 0.6870 - val_loss: 0.6402\nEpoch 6/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7659 - auc: 0.8390 - loss: 0.5119 - val_accuracy: 0.7050 - val_auc: 0.6868 - val_loss: 0.5581\nEpoch 7/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7204 - auc: 0.7949 - loss: 0.5547 - val_accuracy: 0.6350 - val_auc: 0.6886 - val_loss: 0.6483\nEpoch 8/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7581 - auc: 0.8251 - loss: 0.5139 - val_accuracy: 0.7050 - val_auc: 0.6907 - val_loss: 0.5739\nEpoch 9/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7866 - auc: 0.8556 - loss: 0.4807 - val_accuracy: 0.6650 - val_auc: 0.6881 - val_loss: 0.6146\nEpoch 10/10\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8057 - auc: 0.8857 - loss: 0.4383 - val_accuracy: 0.6550 - val_auc: 0.7022 - val_loss: 0.6621\nValidation AUC (edges): 0.7021\n\nEnhancement comparison (AUC):\n   edges: 0.7021\n    none: 0.6782\n   clahe: 0.6668\n unsharp: 0.6506\n\nBest enhancement: edges (AUC=0.7021)\nClass weight: {0: 0.68140589569161, 1: 1.878125}\nEpoch 1/10\n\u001b[1m74/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5923 - auc: 0.5819 - loss: 0.6743","output_type":"stream"},{"name":"stderr","text":"2026-02-25 01:22:36.248420: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:36.386097: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:36.619438: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:36.755906: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:36.985435: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:37.127644: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:37.433700: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:37.569959: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:38.280706: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-02-25 01:22:38.416477: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 457ms/step - accuracy: 0.5916 - auc: 0.5809 - loss: 0.6751 - val_accuracy: 0.5750 - val_auc: 0.5235 - val_loss: 0.6805 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 203ms/step - accuracy: 0.5349 - auc: 0.5319 - loss: 0.7235 - val_accuracy: 0.5850 - val_auc: 0.5335 - val_loss: 0.6816 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 197ms/step - accuracy: 0.5631 - auc: 0.5641 - loss: 0.6941 - val_accuracy: 0.5850 - val_auc: 0.5363 - val_loss: 0.6753 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - accuracy: 0.5650 - auc: 0.5730 - loss: 0.6869 - val_accuracy: 0.5900 - val_auc: 0.5497 - val_loss: 0.6765 - learning_rate: 1.0000e-04\nEpoch 5/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - accuracy: 0.5141 - auc: 0.5844 - loss: 0.6922 - val_accuracy: 0.5900 - val_auc: 0.5543 - val_loss: 0.6728 - learning_rate: 1.0000e-04\nEpoch 6/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 202ms/step - accuracy: 0.5674 - auc: 0.5475 - loss: 0.6873 - val_accuracy: 0.5850 - val_auc: 0.5621 - val_loss: 0.6796 - learning_rate: 1.0000e-04\nEpoch 7/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 193ms/step - accuracy: 0.6102 - auc: 0.6275 - loss: 0.6629 - val_accuracy: 0.6050 - val_auc: 0.5693 - val_loss: 0.6655 - learning_rate: 1.0000e-04\nEpoch 8/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 197ms/step - accuracy: 0.5975 - auc: 0.6217 - loss: 0.6863 - val_accuracy: 0.6050 - val_auc: 0.5746 - val_loss: 0.6672 - learning_rate: 1.0000e-04\nEpoch 9/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - accuracy: 0.6152 - auc: 0.6144 - loss: 0.6686 - val_accuracy: 0.6100 - val_auc: 0.5787 - val_loss: 0.6641 - learning_rate: 1.0000e-04\nEpoch 10/10\n\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 204ms/step - accuracy: 0.6579 - auc: 0.6838 - loss: 0.6300 - val_accuracy: 0.5950 - val_auc: 0.5820 - val_loss: 0.6654 - learning_rate: 1.0000e-04\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\nTEST AUC: 0.6306731259561449\n\nClassification report (threshold=0.5):\n              precision    recall  f1-score   support\n\n           0       0.79      0.69      0.74       148\n           1       0.36      0.49      0.42        53\n\n    accuracy                           0.64       201\n   macro avg       0.58      0.59      0.58       201\nweighted avg       0.68      0.64      0.65       201\n\nConfusion matrix:\n [[102  46]\n [ 27  26]]\n\nSaved: efficientnet_best_edges_500x500.h5\n","output_type":"stream"}],"execution_count":10}]}